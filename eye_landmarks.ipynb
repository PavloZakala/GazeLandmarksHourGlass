{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eye_landmarks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZPavlo/GazeLandmarksHourGlass/blob/main/eye_landmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvQBIcUeIl-0"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "ROOT_DATA = 'gdrive/MyDrive/unityeye/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnODasSwjXV1"
      },
      "source": [
        "import os, sys\n",
        "\n",
        "!git clone https://github.com/ZPavlo/GazeLandmarksHourGlass \n",
        "sys.path.append(\"GazeLandmarksHourGlass\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRkGWyiwNffI"
      },
      "source": [
        "!pip install progress"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kGfLbI94hJV"
      },
      "source": [
        "import torch \n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2fbEJFdKF9S"
      },
      "source": [
        "from model.hourglass import HourglassNet\n",
        "\n",
        "NUM_STACKS = 3\n",
        "NUM_BLOCKS = 4\n",
        "NUM_CLASSES = 17\n",
        "print(\"==> creating model: stacks={}, blocks={}\".format(NUM_STACKS, NUM_BLOCKS))\n",
        "model = HourglassNet(num_stacks=NUM_STACKS, num_blocks=NUM_BLOCKS, num_classes=NUM_CLASSES)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O7mEMaNKiLx"
      },
      "source": [
        "from losses import JointsMSELoss\n",
        "from utils.optimizer import get_optimizer\n",
        "\n",
        "LR = 0.005\n",
        "\n",
        "criterion = JointsMSELoss().to(device)\n",
        "optimizer = get_optimizer(\"adam\", model, lr=LR)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdAiiChXU5tv"
      },
      "source": [
        "from utils.data_preprocessing import EyeLandmarksDataset, TrainDataset, TestDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "unity_eye = EyeLandmarksDataset(ROOT_DATA, load_full=True)\n",
        "train_dataset = TrainDataset(unity_eye, data_config={\n",
        "    \"max_shift\": (5, 7),\n",
        "    \"delta_scale\": 0.4,\n",
        "    \"max_rotation_angle\": 0.5,\n",
        "    \"image_size\": (128, 96),\n",
        "    \"line_count\": 2,\n",
        "    \"down_up_scale\": 0.4,\n",
        "    \"sigma_head_map\": 35.0,\n",
        "})\n",
        "\n",
        "test_dataset = TestDataset(unity_eye, {\n",
        "    \"line_count\": 0,\n",
        "    \"image_size\": (128, 96),\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUGhnz5jK1rg"
      },
      "source": [
        "from train import train, validate\n",
        "from utils.tools import adjust_learning_rate, save_checkpoint\n",
        "lr = LR\n",
        "\n",
        "lrs = []\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_acc_list = []\n",
        "valid_acc_list = []\n",
        "start_epoch = 0\n",
        "best_acc = 0.0"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7413YO2SZ3i"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "NUM_WORKERS = 1\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQJMo0Tr57EP"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "EPOCH_SIZE = 5\n",
        "SNAPSHOT = 1\n",
        "CHECKPOINT_PATH = r\"/content/gdrive/MyDrive/exp4\"\n",
        "\n",
        "if not os.path.isdir(CHECKPOINT_PATH):\n",
        "    os.mkdir(CHECKPOINT_PATH)\n",
        "\n",
        "difficult = np.linspace(0.15, 0.65, EPOCH_SIZE)\n",
        "\n",
        "SCHEDULE = [2, 3, 4]\n",
        "GAMMA = 0.8\n",
        "\n",
        "for epoch in range(start_epoch, EPOCH_SIZE):\n",
        "    lr = adjust_learning_rate(optimizer, epoch, lr, SCHEDULE, GAMMA)\n",
        "    print('\\nEpoch: %d | LR: %.8f' % (epoch + 1, lr))\n",
        "\n",
        "    train_dataset.set_difficult(difficult[epoch])\n",
        "    test_dataset.set_difficult(difficult[epoch])\n",
        "    # train for one epoch\n",
        "    train_loss, train_acc, best_acc = train(train_dataloader, model, criterion, optimizer, train_size=1000,\n",
        "                                            print_step=10, save_step=550, checkpoint_path=CHECKPOINT_PATH,\n",
        "                                            best_acc=best_acc)\n",
        "\n",
        "    # evaluate on validation set\n",
        "    valid_loss, valid_acc, best_acc = validate(test_dataloader, model, criterion, print_step=10, best_acc=best_acc)\n",
        "\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'best_acc': max(valid_acc, best_acc),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }, valid_acc > best_acc, checkpoint=CHECKPOINT_PATH, snapshot=SNAPSHOT)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqLwQDiTWvJL"
      },
      "source": [
        "print(train_losses)\n",
        "print(valid_losses)\n",
        "print(train_acc_list)\n",
        "print(valid_acc_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1nY728p6mhx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}