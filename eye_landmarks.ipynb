{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eye_landmarks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyObN7b4mqwztfBpu/zlbwBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZPavlo/GazeLandmarksHourGlass/blob/main/eye_landmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvQBIcUeIl-0",
        "outputId": "9c6fc6c0-23c2-4bfa-9319-a1fbdfa8c5e7"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "ROOT_DATA = 'gdrive/My Drive/unityeye/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnODasSwjXV1",
        "outputId": "0de1f2cf-7a43-4e07-cdc9-5b8a3895180c"
      },
      "source": [
        "import os, sys\n",
        "\n",
        "!git clone https://github.com/ZPavlo/GazeLandmarksHourGlass \n",
        "sys.path.append(\"GazeLandmarksHourGlass\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GazeLandmarksHourGlass'...\n",
            "remote: Enumerating objects: 60, done.\u001b[K\n",
            "remote: Counting objects: 100% (60/60), done.\u001b[K\n",
            "remote: Compressing objects: 100% (39/39), done.\u001b[K\n",
            "remote: Total 60 (delta 23), reused 50 (delta 16), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (60/60), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRkGWyiwNffI",
        "outputId": "1cddea40-442b-4f73-c83b-57b0ec171a03"
      },
      "source": [
        "!pip install progress"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting progress\n",
            "  Downloading https://files.pythonhosted.org/packages/38/ef/2e887b3d2b248916fc2121889ce68af8a16aaddbe82f9ae6533c24ff0d2b/progress-1.5.tar.gz\n",
            "Building wheels for collected packages: progress\n",
            "  Building wheel for progress (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progress: filename=progress-1.5-cp36-none-any.whl size=8074 sha256=f4ee6dfb1500da526d3e3bf3a9839c7bb1e9b86737e0876bb715b44d4b536290\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/c8/80/32a294e3041f006c661838c05a411c7b7ffc60ff939d14e116\n",
            "Successfully built progress\n",
            "Installing collected packages: progress\n",
            "Successfully installed progress-1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kGfLbI94hJV"
      },
      "source": [
        "import torch \n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2fbEJFdKF9S",
        "outputId": "ff476e8f-0379-4ce7-a686-1b4654906f47"
      },
      "source": [
        "from model.hourglass import HourglassNet\n",
        "\n",
        "NUM_STACKS = 3\n",
        "NUM_BLOCKS = 4\n",
        "NUM_CLASSES = 17\n",
        "print(\"==> creating model: stacks={}, blocks={}\".format(NUM_STACKS, NUM_BLOCKS))\n",
        "model = HourglassNet(num_stacks=NUM_STACKS, num_blocks=NUM_BLOCKS, num_classes=NUM_CLASSES)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> creating model: stacks=3, blocks=4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O7mEMaNKiLx"
      },
      "source": [
        "from losses import JointsMSELoss\n",
        "from utils.optimizer import get_optimizer\n",
        "\n",
        "LR = 0.0001\n",
        "\n",
        "criterion = JointsMSELoss().to(device)\n",
        "optimizer = get_optimizer(\"adam\", model, lr=LR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdAiiChXU5tv",
        "outputId": "cad8f6c4-623d-4a99-bb70-8faf4f5994d5"
      },
      "source": [
        "from utils.data_preprocessing import EyeLandmarksDataset, TrainDataset, TestDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "unity_eye = EyeLandmarksDataset(ROOT_DATA, load_full=True)\n",
        "train_dataset = TrainDataset(unity_eye, data_config={\n",
        "    \"max_shift\": (5, 7),\n",
        "    \"delta_scale\": 0.2,\n",
        "    \"max_rotation_angle\": 0.5,\n",
        "    \"image_size\": (128, 96),\n",
        "    \"line_count\": 2,\n",
        "    \"down_up_scale\": 0.4,\n",
        "    \"sigma_head_map\": 35.0,\n",
        "})\n",
        "\n",
        "test_dataset = TestDataset(unity_eye, {\n",
        "    \"line_count\": 2,\n",
        "    \"image_size\": (128, 96),\n",
        "})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [02:14<00:00, 13.48s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Max shift: (5, 7)\n",
            "Delta scale: 0.2\n",
            "Max rotation angle: 0.5\n",
            "Image size: (128, 96)\n",
            "Line count: 2\n",
            "Down up scale: 0.4\n",
            "Sigma head map: 35.0\n",
            "Max shift: (3, 4)\n",
            "Delta scale: 0.0\n",
            "Max rotation angle: 0.3\n",
            "Image size: (128, 96)\n",
            "Line count: 2\n",
            "Down up scale: 0.0\n",
            "Sigma head map: 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUGhnz5jK1rg"
      },
      "source": [
        "from train import train, validate\n",
        "from utils.tools import adjust_learning_rate\n",
        "lr = LR\n",
        "\n",
        "lrs = []\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_acc_list = []\n",
        "valid_acc_list = []\n",
        "start_epoch = 0\n",
        "best_acc = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7413YO2SZ3i"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "NUM_WORKERS = 1\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQJMo0Tr57EP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c93c3a9-7eef-4ce1-c289-34ed5a8da396"
      },
      "source": [
        "EPOCH_SIZE = 10\n",
        "SNAPSHOT = 2\n",
        "CHECKPOINT_PATH = r\"checkpoints\\exp1\"\n",
        "\n",
        "SCHEDULE = [3, 6, 9]\n",
        "GAMMA = 0.8\n",
        "\n",
        "for epoch in range(start_epoch, EPOCH_SIZE):\n",
        "    lr = adjust_learning_rate(optimizer, epoch, lr, SCHEDULE, GAMMA)\n",
        "    print('\\nEpoch: %d | LR: %.8f' % (epoch + 1, lr))\n",
        "\n",
        "    train_dataset.set_difficult((epoch + 1) / EPOCH_SIZE)\n",
        "    test_dataset.set_difficult((epoch + 1) / EPOCH_SIZE)\n",
        "    # train for one epoch\n",
        "    train_loss, train_acc, best_acc = train(train_dataloader, model, criterion, optimizer, best_acc)\n",
        "\n",
        "    # evaluate on validation set\n",
        "    valid_loss, valid_acc, predictions, best_acc = validate(test_dataloader, model, criterion, best_acc)\n",
        "\n",
        "    # remember best acc and save checkpoint\n",
        "    is_best = valid_acc > best_acc\n",
        "    best_acc = max(valid_acc, best_acc)\n",
        "    save_checkpoint({\n",
        "        'epoch': epoch + 1,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'best_acc': best_acc,\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }, predictions, is_best, checkpoint=CHECKPOINT_PATH, snapshot=SNAPSHOT)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1 | LR: 0.00010000\n",
            "(1/5313) Data: 0.241085s | Batch: 1.237s | Total: 0:00:01 | ETA: 0:00:00 | Loss: 0.0514 | Acc:  0.0000\n",
            "(2/5313) Data: 0.002627s | Batch: 0.801s | Total: 0:00:02 | ETA: 1:49:30 | Loss: 0.0513 | Acc:  0.0079\n",
            "(3/5313) Data: 0.002270s | Batch: 0.870s | Total: 0:00:02 | ETA: 1:30:11 | Loss: 0.0503 | Acc:  0.0053\n",
            "(4/5313) Data: 0.002293s | Batch: 0.816s | Total: 0:00:03 | ETA: 1:25:47 | Loss: 0.0496 | Acc:  0.0058\n",
            "(5/5313) Data: 0.002312s | Batch: 0.840s | Total: 0:00:04 | ETA: 1:22:23 | Loss: 0.0490 | Acc:  0.0046\n",
            "(6/5313) Data: 0.002435s | Batch: 0.843s | Total: 0:00:05 | ETA: 1:20:45 | Loss: 0.0482 | Acc:  0.0039\n",
            "(7/5313) Data: 0.002532s | Batch: 0.928s | Total: 0:00:06 | ETA: 1:19:43 | Loss: 0.0476 | Acc:  0.0056\n",
            "(8/5313) Data: 0.002313s | Batch: 0.824s | Total: 0:00:07 | ETA: 1:20:03 | Loss: 0.0470 | Acc:  0.0058\n",
            "(9/5313) Data: 0.002447s | Batch: 0.872s | Total: 0:00:08 | ETA: 1:19:08 | Loss: 0.0461 | Acc:  0.0051\n",
            "(10/5313) Data: 0.002438s | Batch: 0.793s | Total: 0:00:08 | ETA: 1:18:54 | Loss: 0.0457 | Acc:  0.0046\n",
            "(11/5313) Data: 0.004954s | Batch: 0.915s | Total: 0:00:09 | ETA: 1:18:00 | Loss: 0.0454 | Acc:  0.0042\n",
            "(12/5313) Data: 0.002545s | Batch: 0.882s | Total: 0:00:10 | ETA: 1:17:59 | Loss: 0.0446 | Acc:  0.0051\n",
            "(13/5313) Data: 0.002385s | Batch: 0.853s | Total: 0:00:11 | ETA: 1:15:51 | Loss: 0.0440 | Acc:  0.0047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqLwQDiTWvJL"
      },
      "source": [
        "print(train_losses)\n",
        "print(valid_losses)\n",
        "print(train_acc_list)\n",
        "print(valid_acc_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPXELRo7N3z0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}